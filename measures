from matplotlib.pyplot import isinteractive
import numpy as np
import pandas as pd
from math import sqrt
from pyrsistent import v
# from similatiry.bhattacharyya_paper import bhatta_coef
from scipy.stats import gaussian_kde
from sklearn.neighbors import NearestNeighbors

from scipy.sparse.csr import csr_matrix

import os
import sys

from scipy import sparse
from sklearn.metrics.pairwise import cosine_similarity


def popularity_measure(a, b):
    sum_visits_pois_a = 0.
    sum_visits_pois_b = 0.
    sum_visits_pois_ab = 0.

    a = a[~np.isnan(a)]
    b = b[~np.isnan(b)]

    s1 = set(a)
    s2 = set(b)
    pois_ab = s1.intersection(s2)

    nume = 1 / sum_visits_pois_ab
    deno = sqrt(sum_visits_pois_a * sum_visits_pois_b)

    Fp = nume / deno

    return Fp


def get_user_location_score(user_id, poi_id, users_x_locations, M, similar_users_to_user_id):
    '''
    https://github.com/ashaypathak/Recommendation-system/blob/master/Movie_Recommendation.ipynb
    :param user_id:
    :param item_id:
    :return:
    '''

    if len(similar_users_to_user_id):

        a = similar_users_to_user_id
        if type(a) is pd.Series:
            b = a.squeeze().tolist()
        else:
            b = [x[0] for x in set([r for r in a]) if x[1] >= 0.05]

        c = M.loc[:, poi_id]
        d = c[c.index.isin(b)]
        f = d[d.notnull()]

        if not f.empty:

            index = f.index.values.squeeze().tolist()

            if user_id in M.index:

                try:
                    users_mean = users_x_locations.groupby(by="user_id", as_index=False)['cut_jenks'].mean()
                    avg_user = users_mean.loc[users_mean.user_id == user_id].get('cut_jenks', default=0).values[0]

                    corr = M.loc[user_id]
                    corr = corr.get(index, default=0)
                    # corr = M.loc[user_id].get(index, default=0)
                    # corr = M.loc[user_id, index]
                except (RuntimeError, TypeError, NameError, KeyError):
                    return np.nan

                if type(corr) is pd.Series:
                    if not corr.empty:
                        corr = corr.fillna(0)
                elif type(corr) is np.float64:
                    corr = pd.Series([corr])
                else:
                    return np.nan

                fin = pd.concat([f, corr], axis=1)
                fin.columns = ['adg_score', 'correlation']
                fin['score'] = fin.apply(lambda x: x['adg_score'] * x['correlation'], axis=1)

                numerator = fin['score'].sum()
                denominator = fin['correlation'].sum()
                final_score = avg_user + (numerator / denominator)

                return final_score

    return np.nan


def get_UUCF(users_x_locations):
    similarities = []
    for u in users_x_locations.user_id.unique():
        for v in users_x_locations[users_x_locations.user_id != u].user_id.unique():
            for l in users_x_locations.poi_id.unique():
                r_u_l = users_x_locations[
                    (users_x_locations.user_id == u) & (users_x_locations.poi_id == l)].rating.values
                r_v_l = users_x_locations[
                    (users_x_locations.user_id == v) & (users_x_locations.poi_id == l)].rating.values

                if len(r_u_l):
                    r_u_l = r_u_l[0]
                else:
                    r_u_l = 0

                if len(r_v_l):
                    r_v_l = r_v_l[0]
                else:
                    r_v_l = 0

                cor = r_u_l * r_v_l
                # cor = getULSim(u,v,l)
                if np.isnan(cor):
                    continue
                else:
                    similarities.append((u, v, cor))

    similarities.sort(key=lambda tup: tup[2], reverse=True)
    return similarities  # similarities[:num]


def get_LLCF(users_x_locations, target_u):
    u_sim_uu = get_UUCF(users_x_locations)
    # u_sim_u = filter(lambda x: (x[0] == target_u and x[1] == v), u_sim)

    similarities = []
    for i in users_x_locations.poi_id.unique():
        users_i = users_x_locations[users_x_locations.poi_id == i].user_id.unique()
        for j in users_x_locations[users_x_locations.poi_id != i].poi_id.unique():
            users_j = users_x_locations[users_x_locations.poi_id == j].user_id.unique()
            users_U_ij = set(users_i) + set(users_j)
            for v in users_U_ij:
                r_u_i = \
                    users_x_locations[(users_x_locations.user_id == v) & (users_x_locations.poi_id == i)].rating.values[
                        0]
                r_v_j = \
                    users_x_locations[(users_x_locations.user_id == v) & (users_x_locations.poi_id == j)].rating.values[
                        0]
                sim_u_v = filter(lambda x: (x[0] == target_u and x[1] == v), u_sim_uu)

                cor = sim_u_v * r_u_i * r_v_j
                # cor = getULSim(u,v,l)
                if np.isnan(cor):
                    continue
                else:
                    similarities.append((target_u, i, j, cor))

    similarities.sort(key=lambda tup: tup[3], reverse=True)
    return similarities  # similarities[:num]


def get_CF(users_x_locations, target_u, num_recommendations):
    u_sim_ll = get_LLCF(users_x_locations, target_u)

    similarities = []
    for u in users_x_locations.user_id.unique():
        for i in users_x_locations.poi_id.unique():
            if len(users_x_locations[(users_x_locations.user_id == u) & (users_x_locations.poi_id == i)]) == 0:
                for j in users_x_locations.poi_id.unique():
                    r_u_j = \
                        users_x_locations[
                            (users_x_locations.user_id == u) & (users_x_locations.poi_id == j)].rating.values[
                            0]
                    sim_i_j = filter(lambda x: (x[1] == i and x[2] == j), u_sim_ll)

                    cor = sim_i_j * r_u_j
                    # cor = getULSim(u,v,l)
                    if np.isnan(cor):
                        continue
                    else:
                        similarities.append((target_u, i, j, cor))

    similarities.sort(key=lambda tup: tup[3], reverse=True)
    return similarities[:num_recommendations]


def getULSim(u, v, l):
    return get_SBCF_similarity() * get_SBCF_similarity()


# Rating formulas and handlers

def get_UIR(visits_u_l, time_visits_u_l, total_visits_u, total_visits_l, num_visited_places_u):
    return (visits_u_l) / (total_visits_u)


def get_UIR_sqrt(visits_u_l, time_visits_u_l, total_visits_u, total_visits_l, num_visited_places_u, Fp):
    return (visits_u_l * (1 / Fp)) / np.sqrt(total_visits_u * (1 / Fp))


def get_UIR_Brais(visits_u_l, time_visits_u_l, total_visits_u, total_visits_l, num_visited_places_u, Fp):
    '''
    acho que vai a funcionar melhor que o sigmoide
    UIR(u, l) = max(1, .5 * Visits(u, l) * NumberOfVisitedPlaces(u) / TotalVisits(u))

    '''

    # return max(1, .5 * visits_u_l * num_visited_places_u) / (total_visits_u)
    return min(1, .5 * visits_u_l * num_visited_places_u / total_visits_u)


def get_UIR_Brais_sig(visits_u_l, time_visits_u_l, total_visits_u, total_visits_l, num_visited_places_u, Fp):
    def stable_sigmoid(x):
        sig = np.where(x < 0, np.exp(x) / (1 + np.exp(x)), 1 / (1 + np.exp(-x)))
        return sig

    '''
    AsÃ­ guarantiza o valor entre 0 e 1
    UIR(u, l) = sigmoid(Visits(u, l) * NumberOfVisitedPlaces(u) / TotalVisits(u) - 1)

    '''

    return stable_sigmoid((visits_u_l * num_visited_places_u) / (total_visits_u - 1))


# https://dl.acm.org/doi/pdf/10.1145/1989656.1989660
def get_EWI(max_visits_u, min_visits_u, n=10):
    return (max_visits_u - min_visits_u) / n


def get_pearson_recs_by_item(poi_name, M, num):
    reviews = []
    for poi_id in M.columns:
        if poi_id == poi_name:
            continue
        cor = pearson(M[poi_name], M[poi_id])
        if np.isnan(cor):
            continue
        else:
            reviews.append((poi_id, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def get_pearson_recs_by_user(user_id, M, num):
    reviews = []
    for u_id in M.index:
        if u_id == user_id:
            continue
        cor = pearson(M.loc[u_id], M.loc[user_id])
        if np.isnan(cor):
            continue
        else:
            reviews.append((u_id, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def get_user_based_prediction(a, b):
    r_u = np.mean(a)

    pred = r_u


def get_item_based_prediction(a, b):
    r_u = np.mean(a)

    pred = r_u


def loc_med(s1, s2):
    # from scipy import stats
    # r, p_value = stats.pearsonr(s1, s2)
    # return r

    """Take two list objects and return a pearson correlation."""
    try:
        s1_c = s1 - s1.median()
        s2_c = s2 - s2.median()
    except:
        s1_c = s1 - np.median(s1)
        s2_c = s2 - np.median(s2)

    return np.sum(s1_c * s2_c) / np.sqrt(np.sum(s1_c ** 2) * np.sum(s2_c ** 2))


def loc_cor(s1, s2):
    """Take two list objects and return a pearson correlation."""

    try:
        s1_c = s1 - s1.mean()
        s2_c = s2 - s2.mean()
    except:
        s1_c = s1 - np.mean(s1)
        s2_c = s2 - np.mean(s2)

    return np.sum(s1_c * s2_c) / np.sqrt(np.sum(s1_c ** 2) * np.sum(s2_c ** 2))


def jacc(a, b):
    # Getting only the values that were rated by the individuals
    try:
        s1 = set(a)
        s2 = set(b)
        jacc_c = len(s1.intersection(s2)) / len(s1.union(s2))

        return jacc_c
    except:
        a = [np.where(np.array(a, copy=False) > 0)]  # [np.where(a > 0)]
        b = [np.where(np.array(a, copy=False) > 0)]  # [np.where(b > 0)]
        s1 = set(a)
        s2 = set(b)

    return len(s1.intersection(s2)) / len(s1.union(s2))


def jacc_array(a, b):
    # Getting only the values that are valid numbers
    a = a[~pd.isna(a)]
    b = b[~pd.isna(b)]

    # Getting only the values that were rated by the individuals
    # a = [np.where(a > 0)]
    # b = [np.where(b > 0)]
    a = [np.where(~pd.isna(a))]
    b = [np.where(~pd.isna(b))]

    s1 = set(np.unique(a[0], axis=0)[0].tolist())  # set(a)
    s2 = set(np.unique(b[0], axis=0)[0].tolist())  # set(b)
    # s1 = set(a)
    # s2 = set(b)

    return len(s1.intersection(s2)) / len(s1.union(s2))


def jacc_union_array(a, b):
    # Getting only the values that are valid numbers
    a = a[~pd.isna(a)]
    b = b[~pd.isna(b)]

    # Getting only the values that were rated by the individuals
    a = [np.where(a > 0)]
    b = [np.where(b > 0)]

    s1 = set(np.unique(a[0], axis=0)[0].tolist())  # set(a)
    s2 = set(np.unique(b[0], axis=0)[0].tolist())  # set(b)
    # s1 = set(a)
    # s2 = set(b)

    return len(s1.union(s2))


def bc_new(h1, h2):
    '''Calculates the Byattacharyya distance of two histograms.'''

    def normalize(h):
        return h / np.sum(h)

    #distance = -np.log(np.sum(np.sqrt(np.multiply(normalize(h1), normalize(h2)))))
    distance = 1 - np.sum(np.sqrt(np.multiply(normalize(h1), normalize(h2))))

    if np.isinf(distance):
        return 0
    return distance
    # return 1 - np.sum(np.sqrt(np.multiply(normalize(h1), normalize(h2))))


def bc(a, b):
    def normalize(h):
        return h / np.sum(h)

    return np.sum(np.sqrt(np.multiply(normalize(a), normalize(b))))

    # return sum(sqrt(p * q) for p, q in zip(a, b))


def bc_array(a, b):
    # working version
    try:
        return bhatta_coef(X1=a, X2=b, method='autohist')
    except:
        return bhatta_coef(X1=a.data, X2=b.data, method='autohist')

    # return similarity_Bhattacharya(a, b)

    # 1st version
    # def normalize(h):
    #    return h / np.sum(h)
    # return np.sum(np.sqrt(np.multiply(normalize(a), normalize(b))))

    # 2nd version
    # return sum(sqrt(p * q) for p, q in zip(a, b))

    # working version
    if False:
        from dictances import bhattacharyya_coefficient, bhattacharyya

        # convert numpy array to dictionary
        a_d = dict(enumerate(a.flatten(), 1))
        a_d = {k: v for k, v in a_d.items() if ~np.isnan(v)}
        b_d = dict(enumerate(b.flatten(), 1))
        b_d = {k: v for k, v in b_d.items() if ~np.isnan(v)}
        bhatt_s = bhattacharyya_coefficient(a_d, b_d)

        return bhatt_s


def bhatta_coef(X1, X2, method='continuous'):
    # Calculate the Bhattacharyya coefficient between X1 and X2. X1 and X2 should be 1D numpy arrays representing the same
    # feature in two separate classes.

    X1 = X1[~pd.isna(X1)]
    X2 = X2[~pd.isna(X2)]

    def get_density(x, cov_factor=0.1):
        # Produces a continuous density function for the data in 'x'. Some benefit may be gained from adjusting the cov_factor.
        density = gaussian_kde(x)
        density.covariance_factor = lambda: cov_factor
        density._compute_covariance()
        return density

    # Combine X1 and X2, we'll use it later:
    cX = np.concatenate((X1, X2))

    if method == 'noiseless':
        ###This method works well when the feature is qualitative (rather than quantitative). Each unique value is
        ### treated as an individual bin.
        uX = np.unique(cX)
        A1 = len(X1) * (max(cX) - min(cX)) / len(uX)
        A2 = len(X2) * (max(cX) - min(cX)) / len(uX)
        bht = 0
        for x in uX:
            p1 = (X1 == x).sum() / A1
            p2 = (X2 == x).sum() / A2
            bht += sqrt(p1 * p2) * (max(cX) - min(cX)) / len(uX)

    elif method == 'hist':
        ###Bin the values into a hardcoded number of bins (This is sensitive to N_BINS)
        N_BINS = 10
        # Bin the values:
        h1 = np.histogram(X1, bins=N_BINS, range=(min(cX), max(cX)), density=True)[0]
        h2 = np.histogram(X2, bins=N_BINS, range=(min(cX), max(cX)), density=True)[0]
        # Calc coeff from bin densities:
        bht = 0
        for i in range(N_BINS):
            p1 = h1[i]
            p2 = h2[i]
            bht += sqrt(p1 * p2) * (max(cX) - min(cX)) / N_BINS

    elif method == 'autohist':
        ###Bin the values into bins automatically set by np.histogram:
        # Create bins from the combined sets:
        # bins = np.histogram(cX, bins='fd')[1]
        bins = np.histogram(cX, bins='doane')[1]  # Seems to work best
        # bins = np.histogram(cX, bins='auto')[1]

        h1 = np.histogram(X1, bins=bins, density=True)[0]
        h2 = np.histogram(X2, bins=bins, density=True)[0]

        # Calc coeff from bin densities:
        bht = 0
        for i in range(len(h1)):
            p1 = h1[i]
            p2 = h2[i]
            bht += sqrt(p1 * p2) * (max(cX) - min(cX)) / len(h1)

    elif method == 'continuous':
        ###Use a continuous density function to calculate the coefficient (This is the most consistent, but also slightly slow):
        N_STEPS = 200
        # Get density functions:
        d1 = get_density(X1)
        d2 = get_density(X2)
        # Calc coeff:
        xs = np.linspace(min(cX), max(cX), N_STEPS)
        bht = 0
        for x in xs:
            p1 = d1(x)
            p2 = d2(x)
            bht += sqrt(p1 * p2) * (max(cX) - min(cX)) / N_STEPS

    else:
        raise ValueError("The value of the 'method' parameter does not match any known method")

    return bht


def get_SBCF_similarity(M, u, v):
    # return jacc(a, b) + sum(sqrt(p * q) for p, q in zip(a, b)) * loc_med(a, b)

    M_u = M[M.index == u]  # .dropna(axis='columns')
    M_v = M[M.index == v]  # .dropna(axis='columns')
    j_s = jacc(M_u.dropna(axis='columns').columns.to_list(), M_v.dropna(axis='columns').columns.to_list())

    loc_s = pearson(M.loc[u], M.loc[v])  # loc_med(M.iloc[u], M.iloc[v])

    # from dictances import bhattacharyya_coefficient, bhattacharyya
    # bhatt_s = bhattacharyya_coefficient(M.loc[u].to_dict(),  M.loc[v].to_dict())

    bhatt_s = bc_array(M.loc[u], M.loc[v])

    bhatt_s_new = bc_new(M.loc[u], M.loc[v])

    return j_s + (bhatt_s * loc_s)


def get_SBCF_similarity_array(u, v):
    if False:
        print("type of u ", type(u))
        print("get_SBCF_similarity_array, u ", u)

        print("type of v ", type(v))
        print("get_SBCF_similarity_array, v ", v)

    # return jacc(a, b) + sum(sqrt(p * q) for p, q in zip(a, b)) * loc_med(a, b)

    # sys.stdout.write('\rComputing similarities. Progress status : %.1f%%' % (float(count / nb_items)*100.0))
    # sys.stdout.flush()

    j_s = jacc_array(u, v)
    loc_s = pearson_array(u, v)
    bhatt_s = bc_array(u, v)

    return j_s + (bhatt_s * loc_s)


def get_SBCF_adjustment_factor_similarity_array(u, v, adj_factor = 0.75):
    # return jacc(a, b) + sum(sqrt(p * q) for p, q in zip(a, b)) * loc_med(a, b)

    j_s = jacc_array(u, v)
    loc_s = pearson_array(u, v)
    bhatt_s = bc_array(u, v)
    
    # Here we check for the jaccard similarity, if there is no single place in common (jaccard == 0), 
    # we penalize by giving the weight of 1/(1/(union of elements)), which is the denominator of the jaccard similarity, 
    # this tends to return a very small value, so, we separate even more the individuals with no common items
    if j_s > 0:
        weight = (1 / j_s) ** adj_factor
    else:
        weight = (1 / (1 / jacc_union_array(u, v))) ** adj_factor

    return (bhatt_s * loc_s) ** weight


def SBCF_adj(u, v, adj_factor = 0.75):
    j_s = jacc_array(u, v)
    bhatt_s = bc_array(u, v)
    
    weight = (1 / j_s) ** adj_factor

    return bhatt_s ** weight


def get_CF_BI_BU_similarity(a, b):
    # return jacc(a, b) + sum(sqrt(p * q) for p, q in zip(a, b)) * loc_med(a, b)
    j_s = jacc(a, b)
    loc_s = loc_med(a, b)
    bhatt_s = bc(a, b)

    return j_s + (bhatt_s * loc_s)


def get_bhatta_rec_user(u, M, num):
    from dictances import bhattacharyya_coefficient, bhattacharyya

    M_array = np.asarray(M.values)
    M_array = np.round(M_array, 4)

    reviews = []
    for v in M.index:
        if v == u:
            continue

        cor = None
        # cor = bhattacharyya_coefficient(M[poi_id].to_dict(), M[poi_name].to_dict())
        # cor = bhattacharyya(M[poi_id].to_dict(), M[poi_name].to_dict())

        # if len(M[M.index == u].dropna(axis='columns')) and len(M[M.index == v].dropna(axis='columns')):
        #

        cor_SBCF_adjustment_factor_similarity_array = get_SBCF_adjustment_factor_similarity_array(M_array[u], M_array[v])
        cor_SBCF_similarity_array = get_SBCF_similarity_array(M_array[u], M_array[v])

        cor = get_SBCF_similarity(M, u, v)

        if cor is None or np.isnan(cor):
            continue
        else:
            reviews.append((v, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def get_bhatta_rec_item(u, M, num):
    from dictances import bhattacharyya_coefficient, bhattacharyya

    reviews = []
    for v in M.index:
        if v == u:
            continue

        cor = get_SBCF_similarity(M, u, v)

        if np.isnan(cor):
            continue
        else:
            reviews.append((v, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def bhattacharyya_similarity(M, with_adjustment_factor=False):
    # row indices
    row_ind = M.shape[
        0]  # np.array(df_ids[df_ids.directory.isin(df_stay_points.user_id)].directory.sort_values(ascending=True).values)
    # column indices
    col_ind = M.shape[
        1]  # np.array(df_pois[df_pois.poi_id.isin(df_stay_points.poi_id)].poi_id.sort_values(ascending=True).values)

    M_array = np.asarray(M.values)
    M_array = np.round(M_array, 4)
    data, col, row = [], [], []

    for u in range(0, row_ind):
        for v in range(0, row_ind):

            if u == v:
                continue

            if with_adjustment_factor:
                cor = get_SBCF_adjustment_factor_similarity_array(M_array[u], M_array[v])
            else:
                cor = get_SBCF_similarity_array(M_array[u], M_array[v])

            if np.isnan(cor):
                cor = 0

            row.append(u)
            col.append(v)
            data.append(cor)

    sparse_data = sparse.coo_matrix((data, (row, col)), shape=(row_ind, row_ind))
    return sparse_data.toarray()


def pearson_similarity(M):
    # row indices
    row_ind = M.shape[
        0]  # np.array(df_ids[df_ids.directory.isin(df_stay_points.user_id)].directory.sort_values(ascending=True).values)
    # column indices
    col_ind = M.shape[
        1]  # np.array(df_pois[df_pois.poi_id.isin(df_stay_points.poi_id)].poi_id.sort_values(ascending=True).values)

    M_array = np.asarray(M.values)
    M_array = np.round(M_array, 4)
    data, col, row = [], [], []

    for u in range(0, row_ind):
        for v in range(0, row_ind):

            if u == v:
                continue

            cor = pearson_array(M[u], M[v], compress_items=True)

            if np.isnan(cor):
                continue
            else:
                row.append(u)
                col.append(v)
                data.append(cor)

    sparse_data = sparse.coo_matrix((data, (row, col)), shape=(row_ind, row_ind))
    return sparse_data.toarray()


def pearson(s1, s2):
    if False:
        """Take two pd.Series objects and return a pearson correlation."""
        try:
            s1_c = s1 - s1.mean()
            s2_c = s2 - s2.mean()
        except:
            s1_c = s1 - np.mean(s1)
            s2_c = s2 - np.mean(s2)

        return np.sum(s1_c * s2_c) / np.sqrt(np.sum(s1_c ** 2) * np.sum(s2_c ** 2))

    compress_items = True
    try:
        return pd.Series(s1.data).corr(pd.Series(s2.data))
    except:

        if compress_items:
            bad = ~np.logical_or(pd.isna(s1), pd.isna(s2))

            # np.isnan(s2.data).any()
            try:
                s1 = np.compress(bad, s1)
                s2 = np.compress(bad, s2)
            except:
                s1 = s1[~np.isnan(s1)]  # np.compress(~np.isnan(s1), s1)
                s2 = s2[~np.isnan(s2)]  # np.compress(~np.isnan(s2), s2)

        try:
            s1_c = s1 - s1.mean()
            s2_c = s2 - s2.mean()
        except:
            s1_c = s1 - np.mean(s1)
            s2_c = s2 - np.mean(s2)

        return np.sum(s1_c * s2_c) / np.sqrt(np.sum(s1_c ** 2) * np.sum(s2_c ** 2))


def pearson_array(s1, s2, compress_items=True):
    """Take two pd.Series objects and return a pearson correlation."""

    try:
        return pd.Series(s1.data).corr(pd.Series(s2.data))

    except:

        if compress_items:
            bad = ~np.logical_or(pd.isna(s1), pd.isna(s2))

            # np.isnan(s2.data).any()
            try:
                s1 = np.compress(bad, s1)
                s2 = np.compress(bad, s2)
            except:
                s1 = s1[~np.isnan(s1)]  # np.compress(~np.isnan(s1), s1)
                s2 = s2[~np.isnan(s2)]  # np.compress(~np.isnan(s2), s2)

        try:
            s1_c = s1 - s1.mean()
            s2_c = s2 - s2.mean()
        except:
            s1_c = s1 - np.mean(s1)
            s2_c = s2 - np.mean(s2)

        return np.sum(s1_c * s2_c) / np.sqrt(np.sum(s1_c ** 2) * np.sum(s2_c ** 2))


def get_recs(poi_name, M, num):
    reviews = []
    for poi_id in M.columns:
        if poi_id == poi_name:
            continue
        cor = pearson(M[poi_name], M[poi_id])
        if np.isnan(cor):
            continue
        else:
            reviews.append((poi_id, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def get_joao_rec(user_id, poi_name, M, num):
    reviews = []
    for poi_id in M.columns:
        if poi_id == poi_name:
            continue

        ra_ = M.iloc[user_id].mean()
        s2_c = M[poi_id] - M[poi_id].mean()
        cor = ra_ + (np.sum(pearson(M[poi_name], M[poi_id]) * s2_c) / np.sum(pearson(M[poi_name], M[poi_id])))

        if np.isnan(cor):
            continue
        else:
            reviews.append((poi_id, cor))

    reviews.sort(key=lambda tup: tup[1], reverse=True)
    return reviews[:num]


def cosine(x, y):
    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))


def CSA_adj(u, v):
    # Modified Similarity Algorithm for Collaborative Filtering
    """_summary_
    Shen, K., Liu, Y., & Zhang, Z. (2017, August). Modified similarity algorithm for collaborative filtering. 
    In International Conference on Knowledge Management in Organizations (pp. 378-385). Springer, Cham.

    Args:
        u (_type_): _description_
        v (_type_): _description_

    Returns:
        _type_: _description_
    """

    j_s = jacc_array(u, v)
    cos = cosine(u, v)

    adj_factor = 0.75

    weight = (1 / j_s) ** adj_factor

    return cos ** weight


# The lower the better
def get_cosine_sim(user_id, poi_name, M, num):
    M.fillna(0, inplace=True)  # "11/04/2023"

    A = M.to_numpy()  # np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1], [1, 1, 0, 1, 0]])
    A_sparse = sparse.csr_matrix(A)

    similarities = cosine_similarity(A_sparse)
    # print('pairwise dense output:\n {}\n'.format(similarities))

    # also can output sparse matrices
    similarities_sparse = cosine_similarity(A_sparse, dense_output=False)
    # print('pairwise sparse output:\n {}\n'.format(similarities_sparse))

    model_knn = NearestNeighbors(metric='correlation', algorithm='brute')
    # model_knn.fit(A_sparse)
    model_knn.fit(A)

    query_index = user_id  # np.random.choice(M.shape[0])
    query_index = np.random.choice(M.shape[0])
    print('Selected POI index: {}'.format(query_index))

    distances, indices = model_knn.kneighbors(M.iloc[query_index, :].values.reshape(1, -1), n_neighbors=M.shape[0])
    # distances, indices = model_knn.kneighbors(M.iloc[M.user_id == query_index, :].values.reshape(1, -1), n_neighbors=M.shape[0])

    for i in range(0, len(distances.flatten())):
        if i == 0:
            print('Recommendations for {0}: \n'.format(M.index[query_index]))
        else:
            print('{0}: {1} with distance of {2}: \n'.format(i, M.index[indices.flatten()[i]], distances.flatten()[i]))
    # https://github.com/krishnaik06/Recommendation_complete_tutorial/blob/master/KNN%20Movie%20Recommendation/KNNRecommendation.ipynb

    return distances
